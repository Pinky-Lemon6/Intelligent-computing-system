# 任务1
## 实验目的
* 实现`pairwise_distances`函数，计算两个向量之间的$l_2$距离
## 实验环境
* `Python` 3.11.10
* `numpy` 1.26.4
## 实验原理
* $d_2(X, Y)=\sqrt{\sum_{i=1}^n(x_i-y_i)^2}$
## 实验步骤
* 导入所需的库
```python
import numpy as np
```
* 实现函数
    * 定义 `pairwise_distances` 函数，接受两个输入参数 A 和 B，并根据上述公式计算行间距离。
    * 遍历 A 和 B 的每一行，计算每一行之间的差异并累加。
    * 对累加结果开根号得出向量的$l_2$距离。
```python
def pairwise_distances(A, B):
    """return the matrix of distances between the rows of A and the rows of B
    params: A: np.ndarry A.shape = (-1); B: np.ndarry B.shape = (-1)
    A.shape[0] = B.shape[0]"""
    ret = 0
    for i in range(A.shape[0]):
        ret += np.square(A[i]-B[i])
    distance = np.sqrt(ret)
    return distance
```

## 功能测试
* 与numpy自带的$l_2$距离计算函数做对比，随机生成两个d(测试时取d=10)维的向量A和B，分别使用`pairwise_distances` 函数和 `pairwise_distances_numpy` 函数计算A和B之间的$l_2$距离，比较并计算两个距离之间的误差的绝对值。
```python
def pairwise_distances_numpy(A, B):
    """
    directly call the method of calculating vector distance in numpy
    """
    return np.linalg.norm(A - B)

def pairwise_distances_test(d):
    """
    test the function of pairwise_distances(A, B)
    :param d: vector dimension
    """
    A = np.random.rand(d)
    B = np.random.rand(d)
    
    distances_manual = pairwise_distances(A, B)
    distances_numpy = pairwise_distances_numpy(A, B)
    error = abs(distances_manual - distances_numpy)

    print("*"*20,"The functional test of task1","*"*20) 
    print(f"A: {A}")
    print(f"B: {B}")
    print(f"distances_manual: {distances_manual}")
    print(f"distances_numpy: {distances_numpy}")
    print(f"error: {error}")
    print("*"*70) 
```
* 运行结果
```cmd
******************** The functional test of task1 ********************
A: [0.67499076 0.37177926 0.3878624  0.70315142 0.46422767 0.1586337
 0.82279101 0.51921732 0.68716814 0.48589199]
B: [0.00340933 0.53077245 0.32914662 0.4167858  0.74034467 0.61522261
 0.26564728 0.47326819 0.18522302 0.46093013]
distances_manual: 1.1880907969901557
distances_numpy: 1.1880907969901557
error: 0.0
**********************************************************************
```

## 结果分析
* 在实验中， 定义了`pairwise_distances`函数来计算两个向量A和B之间的$L_2$距离，并与numpy库中的`linalg.norm`函数进行了对比测试。测试结果显示，两种方法得到的$L_2$距离值完全一致，误差为0.0，这验证了`pairwise_distances`函数的正确性。这一结果表明，尽管`pairwise_distances`函数是手动实现的，但其计算精度与优化过的库函数相当，能够准确地反映两个向量之间的欧几里得距离。
## 总结与改进
* 通过对`pairwise_distances`函数的测试，证明了该函数能够准确计算两个向量之间的$L_2$距离。这一结果验证了函数实现的正确性，并表明该函数在实际应用中具有实用价值。然而，考虑到计算效率和代码的简洁性，未来的工作可以考虑进一步优化该函数的性能，或者探索其他距离度量方法，以适应不同的应用场景

# 任务2
## 实验目的
* 实现`knn_search`函数，该函数针对一组`query`查询返回在$L_2$距离下`database`中的最近邻。
## 实验环境
* `Python` 3.11.10
* `numpy` 1.26.4
* `sklearn` 1.5.1
* `hnswlib` 0.8.0
## 实验原理
* 定义一个`KNN`类，在其初始化时，使用多个多元正态分布来生成模拟`database`，该`database`由n个维度为d的随机向量组成。
* 随后使用`hnswlib`库中的`create_hnsw_index`方法创建HNSW索引，其中指定距离度量为$L_2$距离。
* 查询时，遍历每个查询向量，使用HNSW索引的`knn_query`方法对查询向量进行最近邻搜索，选出距离最近的k个近邻。
## 实验步骤
* 导入所需要的库
```python
import numpy as np
from sklearn.cluster import KMeans
import warnings
warnings.filterwarnings('ignore')
import hnswlib
import time
```
* 定义`KNN`类并实现`init`函数
  * 参数说明：`n`是数据库中向量的数量，`d`是向量的维度，`k`是查询时要返回的最近邻的数量；
  * 属性初始化：`self.data`是随机生成的向量数据库，`self.index`是对该数据库构建的HNSW索引。
```python
class KNN:
    def __init__(self, n, d, k=1):
        """
        Initialize KNN class
        :param n: number of vectors
        :param d: vector dimension
        :param k: number of neighbors
        """
        self.n = n
        self.d = d
        self.k = k
        self.data = self.generate_vectors()
        self.index = self.create_hnsw_index()
```
* 使用多元正态分布生成模拟`database`
  * 首先随机生成`n_cluster`个聚类中心，并使用单位矩阵作为协方差矩阵；
  * 之后从所有聚类中随机选取一个聚类，生成对应的多元正态分布向量；
  * 函数返回n个d维的符合多元正态分布的随机向量。
```python
def generate_vectors(self, n_clusters=4):
    """
    Generate n d-dimensional vectors using multivariate normal distribution
    :param n_clusters: number of clusters
    :return: generated vector
    """
    means = np.random.rand(n_clusters, self.d)  
    covariances = [np.eye(self.d) for _ in range(n_clusters)]  
    
    vectors = []
    for i in range(self.n):
        cluster = np.random.choice(n_clusters)  
        vector = np.random.multivariate_normal(means[cluster], covariances[cluster])  
        vectors.append(vector)

    return np.array(vectors)
```
* 创建 HNSW 索引
  * 首先初始化 HNSW 索引，指定距离度量为$L_2$距离以及向量的维度为`d`；
  * 随后设置索引的参数，如最大元素数量为`n`；
  * 将生成的向量数据添加到索引中；
  * 设置查询时的`ef`值，以便控制查询时的精度和速度
  * 函数返回对`database`构建的 HNSW 索引。
```python
def create_hnsw_index(self):
    """
    Create HNSW Index
    """
    index = hnswlib.Index(space='l2', dim=self.d)  
    index.init_index(max_elements=self.n, ef_construction=200, M=16)  
    index.add_items(self.data, np.arange(self.n))  
    index.set_ef(50)  
    return index
```
* 查找query的最近邻
  * 首先遍历每个查询向量，使用 HNSW 索引的`knn_query`方法查询其最近邻；
  * 获取最近邻的索引和距离，并根据索引从数据库中提取最近邻向量；
  * 将最近邻的索引和向量存储在列表中，并最终返回。
```python
def knn_search(self, query_vectors):
    """
    Returns the nearest neighbor for a set of query vectors
    :param query_vectors: query vectors, shape (m, d)
    :return: the index and vector of the nearest neighbor
    """
    nearest_indices_list = []
    nearest_vectors_list = []

    for query_vector in query_vectors:
        indices, distances = self.index.knn_query(query_vector, k=self.k)  
        nearest_vectors = self.data[indices.flatten()] 
    
        nearest_indices_list.append(indices.flatten())
        nearest_vectors_list.append(nearest_vectors)

    return nearest_indices_list, nearest_vectors_list
```

## 功能测试
* 创建`test_knn_functionality`函数来对`knn_search`函数的功能实现的正确性进行测试。
  * 接收参数：`n`是数据库中向量的数量，`d`是向量的维度，`k`是查询时要返回的最近邻的数量；
  * 首先设置随机种子并创建`KNN`实例；
  * 随后随机生成5个查询向量，每个向量的维度均为`d`，用于测试KNN查询功能是否正确实现。
  * 调用`knn_search`函数对生成的一组查询向量进行KNN查询，得到每个查询向量的最近邻索引以及对应的向量；
  * 将每个查询向量的最近邻索引和向量的信息打印；
  * 检查返回的最近邻索引和向量的数量是否与查询向量的数量一致，对每个查询向量，检查返回的最近邻索引数量是否等于`k`，并检查最近邻向量的维度是否等于`d`。如果不符合，将抛出异常并显示错误信息； 
  * 如果所有断言都通过，打印测试通过的信息。
```python
def test_knn_functionality(n, d, k):
    """
    functional testing: verifying the correctness of the KNN class
    """
    np.random.seed(123)  
    knn = KNN(n=n, d=d, k=k)  
    query_vectors = np.random.rand(5, d)  

    nearest_neighbors, nearest_vectors = knn.knn_search(query_vectors)  
    print("*"*20,"The functional test of task2","*"*20)
    for i in range(len(query_vectors)):
        print(f"The nearest neighbor indices of vector {i}:", nearest_neighbors[i])
        print(f"The nearest neighbor vectors of vector {i}:", nearest_vectors[i])
               
    
    assert len(nearest_neighbors) == len(query_vectors), "Incorrect number of nearest neighbor indices!"
    assert len(nearest_vectors) == len(query_vectors), "Incorrect number of nearest neighbor vectors!"

    for i in range(len(query_vectors)):
        assert len(nearest_neighbors[i]) == k, f"Incorrect number of nearest neighbor indices for query vector {i}"
        
        assert nearest_vectors[i].shape[1] == d, f"The nearest neighbor vector dimensions for query vector {i} are incorrect"

    print("Functional test passed!")
    print("*"*70) 
```
* 测试结果
```cmd
******************** The functional test of task2 ********************
The nearest neighbor indices of vector 0: [875 191 130]
The nearest neighbor vectors of vector 0: [[ 0.63315623  1.12488809  0.65204412  0.40310061  0.46545857  0.61423195
   0.73297564  1.61824527 -0.23456252  0.64231423]
 [ 1.03889176  1.52815596  0.06197627  0.56688655  0.06596028  1.09119378
   0.76535371  0.87080833  0.01138189  0.43935713]
 [ 0.33383956  1.585407    0.18860224  1.20309006 -0.04964519  0.78861785
   0.34432308  1.23760542 -0.37130547  0.89452502]]
The nearest neighbor indices of vector 1: [720 424 699]
The nearest neighbor vectors of vector 1: [[ 0.8170451   0.74176177  0.35800719  0.64662039  0.75535559  0.21532037
  -0.11246179  0.34691164  0.82436096  0.27906848]
 [ 0.24259271  0.50980486  0.58209977  0.69477268  0.97202151 -0.04994299
   0.38832555  0.77688228  0.60487138  1.39385794]
 [ 0.0409336   1.19339692  1.46532697  1.35131757  0.97034274  0.28440712
  -0.42323546  0.2098722   0.470412    1.14432787]]
The nearest neighbor indices of vector 2: [720 135 445]
The nearest neighbor vectors of vector 2: [[ 0.8170451   0.74176177  0.35800719  0.64662039  0.75535559  0.21532037
  -0.11246179  0.34691164  0.82436096  0.27906848]
 [ 0.14903372  0.52783368  1.09087752  0.14689023  1.38547271  1.01544421
   0.94358087  0.61878217  1.17685514  0.48917462]
 [ 0.32089423 -0.21570717  1.4677441   0.84317111  0.56635074  0.52958316
   0.91392016 -0.5933654   0.62481491 -0.09955504]]
The nearest neighbor indices of vector 3: [191 676 720]
The nearest neighbor vectors of vector 3: [[ 1.03889176  1.52815596  0.06197627  0.56688655  0.06596028  1.09119378
   0.76535371  0.87080833  0.01138189  0.43935713]
 [ 0.63947616  0.143777    0.85091235  0.35986162  0.86117205  0.27735322
   1.21772571 -0.50000896  0.12315656  1.42817326]
 [ 0.8170451   0.74176177  0.35800719  0.64662039  0.75535559  0.21532037
  -0.11246179  0.34691164  0.82436096  0.27906848]]
The nearest neighbor indices of vector 4: [720 498 191]
The nearest neighbor vectors of vector 4: [[ 0.8170451   0.74176177  0.35800719  0.64662039  0.75535559  0.21532037
  -0.11246179  0.34691164  0.82436096  0.27906848]
 [ 0.97728294  0.15217785  0.84469695  0.95263572  0.59690119 -0.2696765
   1.14344747  0.2949946   0.02078902  1.27038689]
 [ 1.03889176  1.52815596  0.06197627  0.56688655  0.06596028  1.09119378
   0.76535371  0.87080833  0.01138189  0.43935713]]
Functional test passed!
**********************************************************************
```

## 性能测试
* 创建`test_knn_performance`函数来对`knn_search`函数的性能进行测试，并与使用Kmeans进行聚类后直接查询的方法的性能表现进行对比。
  * 首先设置随机种子并创建`KNN`实例；
  * 随后随机生成50个查询向量，每个向量的维度均为`d`，用于测试KNN查询的性能。
  * 使用 HNSW 方式来进行查询，记录 HNSW 查询的开始时间，调用`knn_search`方法查询最近邻，并记录结束时间，计算 HNSW 查询的耗时，并打印结果。
  * 随后使用 Kmeans 方式来进行查询，首先在KNN类中添加一个使用 KMeans 进行聚类后直接查询的`kmeans_knn_search`方法。
    * 该方法首先对`database`进行 KMeans 聚类，得到聚类标签与聚类中心；
    * 随后对每个查询向量进行处理，首先计算查询向量与每个聚类中心的$L_2$距离，找到距离最近的聚类中心的索引；
    * 对于最近的聚类中的所有向量，使用`pairwise_distances`函数，分别计算与查询向量的距离，并选出最近的`k`个向量
    * 函数返回值为距离查询向量最近的`k`个向量及其索引
  *  最后记录 KMeans 查询的开始时间，调用`kmeans_knn_search`方法查询最近邻，并记录结束时间，计算 Kmeans 查询的耗时，并打印结果。
```python
def kmeans_knn_search(self, query_vectors, n_clusters=5):
    """
    Use KMeans clustering followed by brute force search to find the nearest neighbors
    :param query_vectors: query vectors, shape (m, d)
    :param n_clusters: number of clusters
    :return: the index and vector of the nearest neighbors
    """
    kmeans = KMeans(n_clusters=n_clusters)
    kmeans.fit(self.data)
    
    nearest_indices_list = []
    nearest_vectors_list = []

    for query_vector in query_vectors:
        distances_to_centers = np.linalg.norm(kmeans.cluster_centers_ - query_vector, axis=1)
        nearest_cluster_index = np.argmin(distances_to_centers)  

        cluster_indices = np.where(kmeans.labels_ == nearest_cluster_index)[0]
        cluster_vectors = self.data[cluster_indices]
        distances = np.array([pairwise_distances(cluster_vectors[i], query_vector) for i in range(len(cluster_vectors))])
        nearest_indices = np.argsort(distances)[:self.k]  
        
        nearest_indices_list.append(cluster_indices[nearest_indices])
        nearest_vectors_list.append(cluster_vectors[nearest_indices])

    return nearest_indices_list, nearest_vectors_list

def test_knn_performance(n, d, k):
    """
    Performance test: measure and compare the time it takes HNSW and Kmeans to query the knn of the same set of vectors
    """
    np.random.seed(123)  

    knn = KNN(n=n, d=d, k=k)  
    query_vectors = np.random.rand(100, d)  
    # HNSW
    start_time_1 = time.time()  
    nearest_neighbors_1, nearest_vectors_1 = knn.knn_search(query_vectors)  
    end_time_1 = time.time()  
    
    print("*"*20,"The perfomance test of task2","*"*20)
    print(f"HNSW query for the nearest neighbors of 100 vectors takes: {end_time_1 - start_time_1:.4f} seconds")
    # Kmeans
    start_time_2 = time.time()  
    nearest_neighbors_2, nearest_vectors_2 = knn.kmeans_knn_search(query_vectors)  
    end_time_2 = time.time()
    print(f"Kmeans query for the nearest neighbors of 100 vectors takes: {end_time_2 - start_time_2:.4f} seconds")
    print("*"*70)
```
* 测试结果
```cmd
******************** The perfomance test of task2 ********************
HNSW query for the nearest neighbors of 100 vectors takes: 0.0078 seconds
Kmeans query for the nearest neighbors of 100 vectors takes: 20.0078 seconds
**********************************************************************
```

## 结果分析
* 在`knn_search`函数的功能测试中，通过创建KNN类的实例并对随机生成的查询向量进行最近邻搜索，成功地返回了每个查询向量的最近邻索引和向量。测试结果表明，`knn_search`函数能够正确地在数据库中找到与查询向量距离最近的k个向量，并且返回的最近邻向量的数量和维度均符合预期，证明了`knn_search`函数得到了正确实现。
* 而在性能测试中，对比了HNSW索引和KMeans聚类后直接查询的性能。测试中，随机生成了100个查询向量，并记录了两种方法查询最近邻所需的时间。结果显示，HNSW索引在查询最近邻时的耗时远低于KMeans聚类后暴力搜索的方法，具体来说，HNSW查询耗时约为0.0078秒，而KMeans查询耗时约为20.0078秒。这表明HNSW索引在处理高维数据的最近邻搜索时具有显著的性能优势。
* 这一显著的性能差异可以归因于以下几个因素：
  * 索引优化：HNSW索引是一种专为高效最近邻搜索设计的算法，它通过构建多级图结构来优化查询过程，从而减少了查询时的计算量。
  * 维度诅咒：在高维空间中，传统的基于距离的搜索算法（如暴力搜索）效率极低，因为高维空间中的距离度量变得不那么有效。HNSW索引通过其优化的数据结构和查询算法，缓解了这一问题。
  * 聚类效率：KMeans聚类本身需要计算所有数据点与聚类中心之间的距离，当数据量较大时，这一过程非常耗时。此外，聚类后的最近邻搜索还需要在每个聚类内部进行，进一步增加了计算量。

## 总结与改进
* 任务2的实验结果表明，基于HNSW实现的`knn_search`方法能够有效地利用HNSW索引进行快速的最近邻搜索，其性能明显优于传统的基于KMeans聚类的最近邻搜索方法。这一结果验证了HNSW索引在高维空间最近邻搜索中的有效性和优越性。
* 尽管`knn_search`方法已经显示出良好的性能，但仍有一些潜在的改进方向：
  * 参数调优：HNSW索引的性能可能受到其参数（如ef_construction和M）的影响。未来的工作可以探索这些参数的最优设置，以进一步提高查询效率。
  * 数据预处理：在构建HNSW索引之前，可以对数据进行预处理，如归一化或降维，以提高索引的构建和查询效率。
  * 多线程/分布式查询：对于大规模数据集，可以考虑使用多线程或分布式计算来进一步提高`knn_search`方法的查询速度。

